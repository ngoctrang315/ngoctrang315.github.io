[
{
	"uri": "http://ngoctrang315.github.io/2-prerequiste/2.1-cloudformation/",
	"title": "CloudFormation",
	"tags": [],
	"description": "",
	"content": "Step 1: Create a CloudFormation File You can create a .yaml or .json file, for example: Athena-Workshop.yaml.\nAWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: \u0026#34;CloudFormation template to create an S3 bucket and Athena Workgroup\u0026#34; Resources: AthenaS3Bucket: Type: \u0026#34;AWS::S3::Bucket\u0026#34; Properties: BucketName: \u0026#34;my-athena-data-bucket\u0026#34; AthenaWorkGroup: Type: \u0026#34;AWS::Athena::WorkGroup\u0026#34; Properties: Name: \u0026#34;MyAthenaWorkGroup\u0026#34; WorkGroupConfiguration: EnforceWorkGroupConfiguration: true ResultConfiguration: OutputLocation: !Sub \u0026#34;s3://${AthenaS3Bucket}/query-results/\u0026#34; Step 2: Deploy the Template in AWS CloudFormation Open AWS Console â†’ CloudFormation\nClick Create Stack â†’ With new resources (standard)\nIn the Prepare template section, select Choose an existing template In Specify template, select Upload a template file, then click Choose file and upload the Athena-Workshop.yaml file, then click Next\nEnter a Stack Name, e.g., athena-workshop, then click Next\nIn Step 3, under Capabilities, check I acknowledge that AWS CloudFormation might create IAM resources with custom names, then click Next and keep the default settings\nClick Submit and wait approximately 5 minutes for CloudFormation to complete resource creation\nStep 3: Verify Created Resources Open AWS CloudFormation console and check for the CREATE_COMPLETE status\nClick on Athena Basics Stack, then navigate to the Outputs tab to find the S3 Bucket Name\nRecommended region: us-east-1 (N. Virginia), but you can deploy in other regions if needed. Do not change any default database parameters in CloudFormation. "
},
{
	"uri": "http://ngoctrang315.github.io/",
	"title": "Data Analytics with Amazon Athena",
	"tags": [],
	"description": "",
	"content": "Data Analytics with Amazon Athena Overview In this workshop, we will explore the features of Amazon Athena and run hands-on labs to demonstrate its capabilities and best practices.\nContents Introduction Prerequisites Athena Basics Cleanup Resources "
},
{
	"uri": "http://ngoctrang315.github.io/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Amazon Athena Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, requiring no infrastructure management, and you pay only for the queries you run. Simply point to your data in Amazon S3, define the schema, and start querying with SQL. Most results are delivered within seconds.\nStart Querying Instantly No setup, no complex ETL required\nWith Athena, you can query data immediately without deploying servers or managing a complex data infrastructure. Just define the schema and use the built-in query editor in AWS Console to analyze your data instantly.\nPay Only for What You Use Pricing based on data scanned\nWith Amazon Athena, you only pay for the data scanned per query. The current pricing is $5 per terabyte scanned. You can optimize costs by 30% to 90% through compression, partitioning, and converting data to columnar formats like Parquet or ORC.\nPowerful, Flexible, Open Standard Supports standard SQL and multiple data formats\nAmazon Athena uses Presto with ANSI SQL, supporting various standard data formats such as CSV, JSON, ORC, Avro, Parquet. Athena is ideal for quick queries but also supports complex analysis, including large joins, window functions, and array processing.\nHigh Performance, Fast Response Quick query execution even on large datasets\nAthena automatically optimizes and executes queries in parallel, ensuring fast results even when analyzing large-scale data. Since it uses Amazon S3 as its backend storage, you benefit from high availability and 11 nines (99.999999999%) durability.\n"
},
{
	"uri": "http://ngoctrang315.github.io/3-athena-basics/3.1-prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Prerequisites Open the AWS CloudFormation console and check for the CREATE_COMPLETE status.\nClick on the Stack Name as shown below and navigate to the Outputs tab. You will see outputs similar to the ones below. Copy them into Notepad or a text editor for later reference, or keep the CloudFormation tab open.\nðŸ”— Useful Links:\nAWS CloudFormation Documentation Athena Workshop Guide AWS Console "
},
{
	"uri": "http://ngoctrang315.github.io/2-prerequiste/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "Preparation Steps Before starting with Amazon Athena, you need to set up the necessary AWS resources:\nAWS CloudFormation Athena Basics Includes the minimum required resources to perform basic Athena tasks.\nIAM User S3 Bucket (Stores data for querying with Athena) Athena Named Queries Content 2.1. CloudFormation\n"
},
{
	"uri": "http://ngoctrang315.github.io/3-athena-basics/3.2-load-data/",
	"title": "Load Datasets",
	"tags": [],
	"description": "",
	"content": "Load Datasets Before we can use Athena to query our datasets, we need to load the data into an S3 bucket.\nLoad Data into Your Account In the top navigation bar, click the CloudShell icon.\nOnce loaded, it will look like this.\nCopy and paste the following commands into the terminal: accountid=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) aws s3 cp s3://ws-assets-prod-iad-r-iad-ed304a55c2ca1aee/9981f1a1-abdc-49b5-8387-cb01d238bb78/v1/csv/customers.csv ./customers.csv aws s3 cp s3://ws-assets-prod-iad-r-iad-ed304a55c2ca1aee/9981f1a1-abdc-49b5-8387-cb01d238bb78/v1/csv/sales.csv ./sales.csv aws s3 cp customers.csv s3://athena-workshop-$accountid/basics/csv/customers/customers.csv aws s3 cp sales.csv s3://athena-workshop-$accountid/basics/csv/sales/sales.csv rm sales.csv customers.csv aws s3 cp s3://ws-assets-prod-iad-r-iad-ed304a55c2ca1aee/9981f1a1-abdc-49b5-8387-cb01d238bb78/v1/parquet/sales.zip ./sales.zip unzip -o sales.zip rm sales.zip aws s3 sync ./sales s3://athena-workshop-$accountid/basics/parquet/sales aws s3 cp s3://ws-assets-prod-iad-r-iad-ed304a55c2ca1aee/9981f1a1-abdc-49b5-8387-cb01d238bb78/v1/parquet/customers.zip ./customers.zip unzip -o customers.zip aws s3 sync ./customers s3://athena-workshop-$accountid/basics/parquet/customers echo \u0026#34;----- done -----\u0026#34; NOTE: If a \u0026ldquo;Safe Paste for multiline txt\u0026rdquo; prompt is displayed, click the PASTE button. Once the commands have completed, the data will have been loaded into an S3 bucket in your account.\nVerify the Data Load To verify the data has been loaded, follow these steps:\nIn the Search bar at the top of the screen, type S3 and click S3 in the search results. Locate the S3 bucket that has been created for the workshop. It will be named: athena-workshop-[your AWS account number] Example: athena-workshop-12345678910\nClick the bucket name and check the following: There is a folder called basics. The basics folder contains CSV and Parquet folders.\nNow the datasets we will use in the lab have been successfully loaded into your account. "
},
{
	"uri": "http://ngoctrang315.github.io/3-athena-basics/",
	"title": "Athena Basics",
	"tags": [],
	"description": "",
	"content": "DataSets To demonstrate the basic Athena capabilities, a simple synthetic data set will be used.\nWe will be using the following datasets for Athena basic labs:\nCustomers - A set of synthetic customer records (~96,000) Sales - A set of synthetic sales records linked back to a customer (~287,000) In this step, we will connect to our EC2 servers, located in both the public and private subnets.\nContent 3.1. Prerequisites\n3.2. Load Datasets\n3.3. Create Tables\n3.4. Create Tables with Glue\n3.5. Create Views\n3.6. Query Results Re-Use\n3.7. Query Results and History\n3.8. ETL with Athena CTAS\n3.9. Athena Workgroups\n3.10. Visualize with QuickSight using Athena\n"
},
{
	"uri": "http://ngoctrang315.github.io/3-athena-basics/3.3-create-tables/",
	"title": "Create Tables",
	"tags": [],
	"description": "",
	"content": "Athena Enable CloudWatch Metrics Let\u0026rsquo;s first enable CloudWatch Metrics for the primary Athena workgroup so we can look at the metrics after running queries during this workshop.\nSteps: From the AWS Services menu, type Athena and go to the Athena Console. In the Athena Console, click the hamburger icon (â˜° - three horizontal lines) on the top left corner. Choose Workgroups and click on the primary workgroup.\nClick on the Edit button.\nScroll down to Settings, enable Publish query metrics to AWS CloudWatch, and click Save changes.\nAthena Interface - Set Athena Results Location In the Athena Console, click the hamburger icon and choose Query Editor. Click on Settings and then click Manage.\nUnder Manage Settings, click the Browse S3 button.\nFind the bucket named: athena-workshop-\u0026lt;account id\u0026gt; (e.g., athena-bucket-12345678910). Click the radio button to the left of the bucket name, then click Choose.\nClick the Save button to save the S3 location.\nAthena Interface - Create Tables and Run Queries Now we will create some new tables and run queries against them.\nClick the Editor tab to display the Athena Query Editor.\nMake sure you are in the default database before running the queries. Check that the Database dropdown shows default.\nIf you don\u0026rsquo;t see the default database, copy the query below into the query editor and click Run:\nSHOW DATABASES; Click the refresh icon in the data section and verify that default appears in the database dropdown. Creating Tables We will create 4 new tables:\ncustomer_csv sales_csv customer_parquet sales_parquet Create customer_csv Table Navigate back to the Query editor.\nClick the + icon to add a new tab.\nCopy and paste the query below into the query editor:\nCREATE EXTERNAL TABLE customers_csv ( card_id bigint, customer_id bigint, lastname string, firstname string, email string, address string, birthday string, country string) ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.mapred.TextInputFormat\u0026#39; OUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\u0026#39; LOCATION \u0026#39;s3://athena-workshop-\u0026lt;account id\u0026gt;/basics/csv/customers/\u0026#39; TBLPROPERTIES ( \u0026#39;areColumnsQuoted\u0026#39;=\u0026#39;false\u0026#39;, \u0026#39;classification\u0026#39;=\u0026#39;csv\u0026#39;, \u0026#39;columnsOrdered\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;compressionType\u0026#39;=\u0026#39;none\u0026#39;, \u0026#39;delimiter\u0026#39;=\u0026#39;,\u0026#39;, \u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;, \u0026#39;typeOfData\u0026#39;=\u0026#39;file\u0026#39;); Click Run. Check the Query Results tab to ensure it shows Completed.\nCreate sales_csv Table Navigate back to the Query editor.\nClick the + icon to add a new tab.\nCopy and paste the query below into the query editor:\nCREATE EXTERNAL TABLE sales_csv( card_id bigint, customer_id bigint, price string, product_id string, timestamp string) ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;,\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.mapred.TextInputFormat\u0026#39; OUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\u0026#39; LOCATION \u0026#39;s3://athena-workshop-\u0026lt;account id\u0026gt;/basics/csv/sales/\u0026#39; TBLPROPERTIES ( \u0026#39;areColumnsQuoted\u0026#39;=\u0026#39;false\u0026#39;, \u0026#39;classification\u0026#39;=\u0026#39;csv\u0026#39;, \u0026#39;columnsOrdered\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;compressionType\u0026#39;=\u0026#39;none\u0026#39;, \u0026#39;delimiter\u0026#39;=\u0026#39;,\u0026#39;, \u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;, \u0026#39;typeOfData\u0026#39;=\u0026#39;file\u0026#39;); Click Run. Check the Query Results tab to ensure it shows Completed.\nCreate customer_parquet Table Navigate back to the Query editor.\nClick the + icon to add a new tab.\nCopy and paste the query below into the query editor:\nCREATE EXTERNAL TABLE `customers_parquet`( card_id bigint, customer_id bigint, lastname string, firstname string, email string, address string, birthday string) PARTITIONED BY ( `country` string) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\u0026#39; OUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\u0026#39; LOCATION \u0026#39;s3://athena-workshop-\u0026lt;account id\u0026gt;/basics/parquet/customers/\u0026#39; TBLPROPERTIES ( \u0026#39;classification\u0026#39;=\u0026#39;parquet\u0026#39;, \u0026#39;compressionType\u0026#39;=\u0026#39;none\u0026#39;, \u0026#39;partition_filtering.enabled\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;typeOfData\u0026#39;=\u0026#39;file\u0026#39;); MSCK REPAIR TABLE customers_parquet; SHOW PARTITIONS customers_parquet; Highlight the CREATE TABLE query.\nClick Run.\nCheck the Query Results tab to ensure it shows Completed.\nSelect the text MSCK REPAIR TABLE customers_parquet and click Run to add partitions. Once complete, select the text SHOW PARTITIONS customers_parquet and click Run.\nCreate sales_parquet Table Navigate back to the Query editor.\nClick the + icon to add a new tab.\nCopy and paste the query below into the query editor:\nCREATE EXTERNAL TABLE `sales_parquet`( card_id bigint, customer_id bigint, price double, product_id string, timestamp string) PARTITIONED BY ( year string, month string) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\u0026#39; STORED AS INPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\u0026#39; OUTPUTFORMAT \u0026#39;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\u0026#39; LOCATION \u0026#39;s3://athena-workshop-\u0026lt;account id\u0026gt;/basics/parquet/sales/\u0026#39; TBLPROPERTIES ( \u0026#39;classification\u0026#39;=\u0026#39;parquet\u0026#39;, \u0026#39;compressionType\u0026#39;=\u0026#39;none\u0026#39;, \u0026#39;partition_filtering.enabled\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;typeOfData\u0026#39;=\u0026#39;file\u0026#39;); MSCK REPAIR TABLE sales_parquet; SHOW PARTITIONS sales_parquet; Highlight the CREATE TABLE query.\nClick Run.\nCheck the Query Results tab to ensure it shows Completed.\nSelect the text MSCK REPAIR TABLE sales_parquet and click Run to add partitions. Once complete, select the text SHOW PARTITIONS sales_parquet and click Run.\nPartitioning Data in Athena By partitioning your data, you can restrict the amount of data scanned by each query, improving performance and reducing cost.\nAthena supports Apache Hive-style partitions, where paths contain key-value pairs (e.g., country=us/ or year=2021/month=01/day=26/). To load new Hive partitions into a partitioned table, use:\nMSCK REPAIR TABLE table_name; Compare Performance Between Tables Now that we have created the tables, let\u0026rsquo;s run queries to compare their performance.\nSteps: Navigate back to the Query editor.\nClick the + icon to add a new tab.\nCopy and paste the query below into the query editor:\n/* Lets find the top 10 products for Australia from the CSV table*/ select c.country, s.product_id, count(timestamp) as total_transactions from customers_csv c join sales_csv s on c.customer_id = s.customer_id where country =\u0026#39;Australia\u0026#39; group by c.country,s.product_id order by 1,3 desc limit 10; /* Lets find the top 10 products for Australia from the parquet tables*/ select c.country, s.product_id, count(timestamp) as total_transactions from customers_parquet c join sales_parquet s on c.customer_id = s.customer_id where country =\u0026#39;Australia\u0026#39; group by c.country,s.product_id order by 1,3 desc limit 10; /* Lets find the top 10 biggest spending customers from the sales CSV table */ select customer_id, sum(cast(price as decimal(6,2))) as total_sales from sales_csv s group by customer_id order by 2 desc limit 10; /* Lets find the top 10 biggest spending customers from the sales parquet table */ select customer_id, sum(cast(price as decimal(6,2))) as total_sales from sales_parquet s group by customer_id order by 2 desc limit 10; Run the queries one by one, highlighting each SQL statement block and clicking Run.\nIn the Query Results section, note the Run Time and Data Scanned values.\nPerformance Comparison Query Table Time Taken Data Scanned Top Ten Products by transaction count in Australia customers_csv, sales_csv 2.07 sec 559 MB Top Ten Products by transaction count in Australia customers_parquet, sales_parquet 2.40 sec 199 MB Top Ten Customers By Total Spend sales_csv 1.27 sec 525 MB Top Ten Customers By Total Spend sales_parquet 1.7 sec 96 MB "
},
{
	"uri": "http://ngoctrang315.github.io/3-athena-basics/3.4-create-tables-with-glue/",
	"title": "Create Tables with Glue",
	"tags": [],
	"description": "",
	"content": "In this lab, we will use AWS Glue Crawlers to scan sales data and create a new table in the AWS Glue Data Catalog. Then, we will use Amazon Athena to query that table.\nStep 1: Access AWS Glue Console Open the AWS Management Console. Type Glue in the search bar and select AWS Glue from the search results to open the AWS Glue Console. Step 2: Create Crawler In the AWS Glue Console, select Crawlers from the left-hand menu. Click the Create Crawler button. Step 3: Name and Configure Data Source Enter the name Athena Sales for the crawler and click NEXT. When asked \u0026ldquo;Is your data already mapped to Glue tables?\u0026rdquo;, select Not yet.\nClick Add a Data Source. In the Data Source section, select S3.\nClick Browse S3, then follow these steps: Select a bucket that starts with athena-workshop-. Navigate to the Basics folder, then select the parquet folder. Click the circle next to the sales folder. Click Choose. Keep the remaining settings as is and click Add an S3 Data source. Step 4: Create IAM Role Click Next. Click the Create new IAM Role button, enter AWSGlueServiceRole-salescrawler as the role name, and click the Create button. Click the Next button. In the Set output and scheduling section, follow these steps: Target Database: Choose default. Table Name Prefix (optional): Enter athena_glue_. Click Next. Step 5: Review and Create Crawler On the Review and Create screen, click Create Crawler. After the crawler is created, click Run crawler. This process may take 2-4 minutes to complete. After the crawler completes, you will see a new table added. Query the New Table with Athena Once the table is created successfully, we will go back to the Athena Console and run a query.\nStep 6: Access Athena Console Open the AWS Management Console and type Athena in the search bar. Select Athena from the search results to open the Athena Console. Step 7: Query the Table In the left-hand pane of the Query Editor, you will see the athena_glue_sales table in the list of tables. Click the three-dot icon (ellipsis) next to the table name and select Preview Table. The query will run and return sales data. Congratulations! You have successfully created a new table using AWS Glue Crawler and queried it with Amazon Athena.\n"
},
{
	"uri": "http://ngoctrang315.github.io/4-cleanup/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "After completing the workshop, clean up all AWS resources created using AWS CloudFormation by following these steps:\n1. Delete Data in the S3 Bucket CloudFormation can only delete an S3 Bucket if it is empty, so you need to clean up the data first:\nOpen the AWS S3 Console.\nSelect the bucket named athena-workshop-[AWS account number] (e.g., athena-workshop-12345677890).\nClick Empty.\nThen confirm permanently delete, and click Empty.\n2. Delete the CloudFormation Stack Open the AWS CloudFormation Console.\nSelect the stack named athena-workshop. Click Delete and confirm the deletion.\n"
},
{
	"uri": "http://ngoctrang315.github.io/3-athena-basics/3.5-create-views/",
	"title": "Create views",
	"tags": [],
	"description": "",
	"content": "Create Views A view in Amazon Athena is a logical, not a physical table. The query that defines a view runs each time the view is referenced in a query. You can create a view from a SELECT query and then reference this view in future queries.\nFor more information, see CREATE VIEW.\nCreating a Customer Lifetime Value View We are now going to create a new Customer Lifetime Value view in Athena.\nSteps: Navigate back to the Query editor.\nClick the + icon to add a new tab.\nCopy and paste the query below into the query editor:\nCREATE OR REPLACE VIEW \u0026#34;customer_lifetime_value\u0026#34; AS /* Create view for customer life time value */ select c.customer_id,concat(\u0026#34;firstname\u0026#34;,\u0026#39; \u0026#39;,\u0026#34;lastname\u0026#34;) as full_name,c.country, sum(cast(price as decimal(6,2))) as lifetime_value from customers_parquet c join sales_parquet s on c.customer_id = s.customer_id group by c.customer_id,concat(\u0026#34;firstname\u0026#34;,\u0026#39; \u0026#39;,\u0026#34;lastname\u0026#34;),c.country; Select * from customer_lifetime_value limit 10; The query has two parts:\nThe first part creates the customer lifetime value view. The second part selects data from the view. Highlight the first query and click the Run button. This will create a new view called customer_lifetime_value.\nOnce the view is created, it will appear under Views in the resource navigator.\nSelect the second query and click the Run button to select the first 10 records from the view.\nConclusion You have now successfully used Amazon Athena to create a view in your AWS Glue Data Catalog.\n"
},
{
	"uri": "http://ngoctrang315.github.io/3-athena-basics/3.6-query-results-re-use/",
	"title": "Query Results Re-Use ",
	"tags": [],
	"description": "",
	"content": "When you re-run a query in Athena, you can optionally choose to reuse the last stored query result. This option can increase performance and reduce costs in terms of the number of bytes scanned.\nReusing query results is useful if you know that the results will not change within a given time frame. You can specify a maximum age for reusing query results. Athena will use the stored result as long as it is not older than the specified age.\nVerify the Reuse Query Results Feature Open the Athena console by typing Athena in the search bar at the top of the screen. Select Athena from the search results. Check if the Reuse query results option is already enabled: Click the Reuse query results toggle button. If it is enabled, skip to the Testing the reuse query results feature section. Otherwise, complete the following steps to enable the feature.\nEnable Query Results Reuse Select Workgroups under the Administration menu on the left sidebar. Click on the primary workgroup from the list of workgroups.\nClick the Edit button in the top-right corner to edit the workgroup settings.\nIn the Upgrade Query Engine section: Click the Manual radio button. Select Athena engine version 3 from the Query engine version dropdown.\nClick the Save Changes button in the bottom-right corner.\nTesting the Reuse Query Results Feature Steps: Navigate back to the Query editor.\nClick the + icon to add a new tab. Copy and paste the query below into the query editor:\nSELECT * FROM default.customers_csv LIMIT 10; Click the Reuse query results toggle to the off position.\nClick the Run button.\nNote down the Run time and Data scanned values.\nClick the Reuse query results toggle to turn it on. Click the edit icon (pencil) under Reuse query results.\nSet the reuse time to 10 minutes, then click Confirm.\nRe-run the query by clicking Run again.\nCompare the Run time and Data scanned values with the original run.\nGo to the Recent Queries tab in the query editor.\nCheck the Cache column to see the Result reuse value added to the execution.\nNote: You may need to scroll across to find the Cache column.\nRe-run the query with different reuse time\u0026rsquo;s and verify the behaviour. For example change the reuse time from 30 minutes to 1 minute using the You have now successfully explored the query results reuse feature in Amazon Athena.\n"
},
{
	"uri": "http://ngoctrang315.github.io/3-athena-basics/3.7-query-results-and-history/",
	"title": "Query Results and History",
	"tags": [],
	"description": "",
	"content": "Amazon Athena automatically stores query results and metadata in an Amazon S3 location that you can specify. These results can be accessed, downloaded, and reviewed in the Athena console.\nViewing Query History Click on Recent Queries in the Athena console. View all queries submitted in this workgroup, including their State, Run Time, and Data Scanned.\nSelect a Query ID and click Download results to download the query output as a CSV file.\nLocating Query Results in S3 In the AWS Search Bar, type S3, then select S3 from the search results.\nLocate the bucket named athena-workshop-[your AWS account number] Example: athena-workshop-12345678910\nClick on the bucket name to open it. Understanding the Query Results Storage The S3 bucket will contain: A prefix for each saved query that has been run. An Unsaved folder storing results from unsaved queries. Note: If you don\u0026rsquo;t see the Unsaved folder, copy a previous query into a new query tab, run it, and then check the S3 bucket again. Navigating Query Results Click on the Athena_create_coustomer_csv prefix. Click through the Year, Month, and Day prefixes. Year Month Day The results will be stored with a unique Query ID. "
},
{
	"uri": "http://ngoctrang315.github.io/3-athena-basics/3.8-etl-with-athena-ctas/",
	"title": "ETL with Athena CTAS",
	"tags": [],
	"description": "",
	"content": "Raw data in the data lake is typically in CSV or text format, which is not optimized for querying using Athena and other tools. Therefore, converting the data into columnar formats like Parquet is essential. In this lab, we will use the Create Table As Select (CTAS) statement to create a new table from an existing table in CSV format. The CTAS statement will create a new table in Parquet format, compress and partition the data, and then load the data into the new table.\nStep 1: Query and Run CTAS Open Query editor. Click the + to add a new tab. Copy and run the table creation statement.\nThis query has two statements: The first statement will create a new table called customer_sales_ctas. The table format is set to Parquet, a columnar format optimized for performance.\nThe files will be stored in an external location, in this case, the S3 bucket created for the workshop with the prefix /basics/parquet/sales_ctas/.\nData will be partitioned by year and month.\nData is selected from existing tables to populate the new table.\nRun the first statement: Click Run to create the new table. This may take a few minutes. Once the query is complete, you will see a Completed status in the query result tab.\nStep 2: Query the New Table Run the second statement: Select the second statement and click Run to query the newly created table.\nCongratulations! You have successfully created a new table in your data lake and populated it using the Create Table As Select (CTAS) statement in Athena.\n"
},
{
	"uri": "http://ngoctrang315.github.io/3-athena-basics/3.9-athena-workgroups/",
	"title": "Athena Workgroups",
	"tags": [],
	"description": "",
	"content": "Athena Workgroups Amazon Athena allows you to use workgroups to separate users, teams, applications, or workloads. Workgroups enable you to set limits on the amount of data each query or workgroup can process and help track costs. Additionally, workgroups act as resources, allowing you to apply resource-level identity-based policies to control access to a specific workgroup.\nViewing CloudWatch Metrics In the Create Tables lab, we enabled CloudWatch metrics for the primary workgroup. Now, let\u0026rsquo;s review the CloudWatch metrics for the primary workgroup in the Athena console.\nOpen the Athena console. Select Workgroups from the left-hand menu. Click on the primary workgroup from the list of workgroups. Navigate to the Metrics tab in the Workgroup details screen. You will see several metrics, including: Total data scanned Total successful queries Total failed queries Total execution time The displayed graphs may have limited data points, as they reflect the usage of your lab account.\nEnforcing Cost Constraints Athena SQL query pricing is based on the amount of data scanned. Workgroups allow you to enforce cost constraints by setting a maximum data scan limit for queries.\nWe will now configure a workgroup to restrict the maximum data scanned per query.\nIn the Athena console, select Workgroups from the left-hand menu. Click WorkgroupA to view its details. Go to the Data Usage Controls tab and click Manage. Set the maximum data scanned per query in this workgroup to 15 Megabytes (MB). Click Save. Testing the Workgroup Limit To test the new workgroup limit, log in as a new user. You will first retrieve the login credentials from AWS Secrets Manager.\nIn the AWS Management Console, search for CloudFormation and select it. Click the workshop stack name (e.g., athena-workshop). Navigate to the Outputs tab.\nClick the ConsolePassword link to open AWS Secrets Manager. Click on the Secret Name link. Click Retrieve secret value and copy the password to your clipboard. Click on the copy icon to copy the password to clipboard Return to the CloudFormation Outputs tab and click the ConsoleLogin link to open the AWS login page. Enter the following details:\nIAM User Name: userA Password: Paste the copied password. Click Sign in. Once logged in as UserA:\nOpen the Athena console by searching for \u0026ldquo;Athena\u0026rdquo; in the AWS search bar. Click Launch Query Editor. On the Athena dashboard click the \u0026ldquo;Launch query editor\u0026rdquo; button. If you get the prompt below click the Acknowledge button Select WorkgroupA from the Workgroup dropdown. Copy and paste the following queries into the query editor:\n/* This query will fail as it exceeds the 15 MB limit */ SELECT * FROM customers_csv; /* This query will succeed as it only scans 12 MB of data */ SELECT * FROM customers_parquet; Highlight the first query and click Run. The query fails with a Bytes scanned limit exceeded error since it surpasses the 15 MB limit. Highlight the second query and click Run. The query succeeds as it scans less than 15 MB of data. Using Workgroups to Isolate Queries Workgroups allow you to isolate queries for different users, teams, or applications and enforce different query limits. IAM policies control access to specific workgroups.\nTo demonstrate this, attempt to switch workgroups as UserA:\nIn the Workgroups dropdown, select WorkgroupB.\nAn Error fetching workgroup message may appear because UserA lacks permissions to access WorkgroupB. Copy and paste the previous queries into the query editor.\nHighlight the first query and click Run.\nAn error appears since UserA does not have permission to execute queries in WorkgroupB. This demonstrates how Athena Workgroups can be used to isolate queries for different users, teams, and applications. Additionally, workgroup data limits help control query costs effectively.\n"
},
{
	"uri": "http://ngoctrang315.github.io/3-athena-basics/3.10-visualize-with-quicksight-using-athena/",
	"title": "Visualize with QuickSight using Athena",
	"tags": [],
	"description": "",
	"content": "In this lab, we will build an Amazon QuickSight Dashboard that uses Amazon Athena to access data.\nImportant Before setting up QuickSight, we need to log out of AWS and log back in with a user that has the appropriate permissions.\nTo log out: Click the username in the top right-hand corner of the page. Click the Sign Out button. Log back into your AWS Console as an IAM user that has permissions to set up QuickSight.\nSet up QuickSight Once logged back in, follow these steps to set up Amazon QuickSight:\nTake note of the AWS Region where you are running the lab. This is shown in the top menu bar.\nSearch for QuickSight in the AWS search bar and select QuickSight from the results. This will take you to the QuickSight landing page.\nIf your AWS account is not signed up for QuickSight, click Sign up for QuickSight.\nOn the Sign Up for QuickSight screen, complete the following fields:\nEmail for Account Notifications: Enter an email address. QuickSight Region: Choose the region matching your lab setup.\nQuickSight Account Name: Enter a unique name (only letters, numbers, and dashes are allowed).\nIn the QuickSight Access to AWS Services section:\nCheck Amazon Athena. Check Amazon S3 (A popup will appear to select S3 buckets). Click Select All, then Finish.\nIn the Optional Add-On section, uncheck \u0026ldquo;Add Pixel-Perfect reports\u0026rdquo;.\nClick the Finish button.\nOnce completed, you should see an Account created successfully message.\nClick Go to QuickSight to load QuickSight.\nCreate a QuickSight Dataset We will create a new dataset for our QuickSight dashboard that connects to a table in Athena.\nClick New Analysis (top right corner).\nClick New dataset (top left corner).\nLocate the Athena icon and click it.\nFor Data source name, type athenaimmersion and click Create Data Source.\nOn the next screen, select the default database and choose athena_glue_sale table. Click Select.\nOn the finish data set creation screen, choose Query your data directly and click Visualize.\nYou will now see the QuickSight analysis editor.\nCreate QuickSight Charts We will create a simple analysis using our price dataset.\nCreate a Bar Chart From Fields List, drag the month field onto AutoGraph. QuickSight will automatically select a Horizontal Bar Chart displaying count of price records by month.\nCreate a Line Chart Click in the grey space below the existing Bar Chart.\nSet the Value as month, and the Color as year.\nThe chart will display the data as shown below:\nRepeat this process to display the data for price by month and year.\nTry changing the chart type and settings as desired.\nConclusion You have successfully used Amazon QuickSight to connect to your data via Amazon Athena and created a visual analysis.\n"
},
{
	"uri": "http://ngoctrang315.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://ngoctrang315.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]